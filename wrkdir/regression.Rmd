---
title: "Assignment 3"
output: html_notebook
---
Problem 1 - Predicting number of college applications
1. (1p) Download the dataset college.csv and explore its overall structure. Get a summary statistics of each
variable. Answer the following questions:
• How many observation do you have in the data?
• How many categorical and numeric variables you have in your data?

```{r}
clg_df = read.csv("College.csv")
str(clg_df)
```

There are 777 observations, 2 categorical variables and 17 numeric variables.

• Is there any missing value?
```{r}
sum(is.na(clg_df))
```
There appear to be no missing values in the dataset

2. (1pt) Remove the first column (the name of the college)
```{r}
clg_df = clg_df[,-1]
```

3. (2 pts) Which variables are associated with “Apps”. Use “pairs”, “cor”, and side by side box plot with
t.test to answer this question.

```{r}
clg_df = as.data.frame(unclass(clg_df), stringsAsFactors = TRUE)
attach(clg_df)
```
Converting strings to factors and attaching the dataframe
```{r}
iqr = 3264 - 776
iqr_index = (Apps > 776 - 1.5*iqr & Apps < 3264 + 1.5*iqr)
Apps_so = Apps[iqr_index] #so means sans outliers
Private_so = Private[iqr_index]
plot(Apps_so~Private_so)
t.test(Apps_so~Private_so)
```
```{r}
pairs(clg_df[2:6])
pairs(clg_df[c(2,7:9)])
pairs(clg_df[c(2,10:12)])
pairs(clg_df[c(2,13:15)])
pairs(clg_df[c(2,16:18)])
```

```{r}
cor(Apps, clg_df[ ,-c(1, 2)])
```

From the above tests I've concluded that Accept, Enroll, F.Undergrad, Private, Top25perc, P.Undergrad, Outstate, PhD, and Terminal are mildly to highly associated with Apps

4. (1pt) plot the histogram of the number of applications “Apps” variable. Explain what the histogram shows?
```{r}
hist(Apps)
```

Judging from the shape this historgram is right-skewed (possibly j-shaped if you see the small hump between 45k and 50k) with a mean between 0 and 10000

5. (1pt) replace the “Top10perc” variable with a factor variable “Elite” with two levels: “Yes” if 50% or more of new students coming from the top 10% of their high school class (that is, if Top10Perc>=50) , and “No” if less than 50% of new students are coming from the top10% of their high school class.

```{r}
clg_df$Elite = factor(ifelse(clg_df$Top10perc >= 50, "Yes", "No"))
clg_df = clg_df[-5]
```

6. (1pt) is the Elite variable you created above associated with the “Apps” variable? Don’t just say yes or no, you should explain your answer by plots and test statistics.

```{r}
Elite_so = clg_df$Elite[iqr_index]
plot(Apps_so~Elite_so)
t.test(Apps_so~Elite_so)
```

Based on the side-by-side box plot and the p-value = 0.01% < 5% it follows that Elite is associated with Apps

7. (1pt) normalize all numerical features (except the “Apps” variable) using z-score standardization using “scale” function
```{r}
clg_norm = as.data.frame(scale(clg_df[-c(1,2,18)], center=TRUE, scale=TRUE))
clg_norm$Private = clg_df$Private
clg_norm$Apps = clg_df$Apps
clg_norm$Elite = clg_df$Elite
```

8. set the random seed: set.seed(123)
```{r}
set.seed(123)
```

9. (2 pt) Use caret package to run 10 fold cross validation using linear regression method on all features.

Print the resulting model to see the cross validation RMSE. 
In addition, take a summary of the model and interpret the coefficients. 
Which coefficients are statistically different from zero? What does this meant?
```{r}
#install caret package and use the library
install.packages("caret")
library(caret)
```
```{r}
train.control = trainControl(method = "cv", number = 10)
model = train(Apps ~ ., data = clg_norm, method = "lm", trControl = train.control)
```
```{r}
model
summary(model)
```
According to the p-test [Pr(>|t|)] results Accept, Enroll, and EliteYes are statistically different from zero, which means that the true values for these coefficients are unlikely to be zero

10. Set the random seed again. We need to do this before each training to ensure we get the same folds in cross validation. Set.seed(123) so we can compare the models using their cross validation RMSE.
```{r}
set.seed(123)
```

11. (3 pts) Use caret and leap packages to run a 10 fold cross validation using step wise linear regression method with backward selection (i.e., method=”leapBackward). The train method by default uses maximum of 4 predictors and reports the best models with 1..4 predictors. We need to change this parameter to consider 1..16 predictors. So inside your train function, add the following parameter tuneGrid = data.frame(nvmax = 1:16) . Which model (with how many variables or nvmax ) has the lowest RMSE. Take the summary of the final model to see which variables are selected in the model with the lowest RMSE?

```{r}
train.control = trainControl(method = "cv", number = 10)
step.model = train(Apps ~ ., data = clg_norm, method = "leapBackward", trControl = train.control, tuneGrid = data.frame(nvmax = 1:16))
```
```{r}
step.model
summary(step.model$finalModel)
```
Accept, Expend, and EliteYes were selected

12. (2 pt) Split the data into train and test ( you can use the first 621 rows for training and rest for testing). use
“rpart” function to create a regression tree model from the training data. Get the predictions on testing data
and compute the RMSE.

```{r}
install.packages("rpart")
library(rpart)
```

```{r}
clg_train = clg_df[1:621, ]
clg_test = clg_df[622:777, ]
m.rpart = rpart(Apps ~ ., data = clg_train)
p.rpart = predict(m.rpart, clg_test)
(mean((clg_test$Apps - p.rpart)^2))^0.5
```
The RMSE is 1448.4

Problem 2 - Predicting loan default
1- Set the random seed, set.seed(123), and split the data to train/test as we did in slide 37 of week6
lectures.
```{r}
credit = read.csv("credit.csv", stringsAsFactors = TRUE)
credit_norm = as.data.frame(scale(credit[-c(1,3,4,6,7,11,12,14,16,17)], center=TRUE, scale=TRUE))
credit_norm$checking_balance = credit$checking_balance
credit_norm$credit_history = credit$credit_history
credit_norm$purpose = credit$purpose
credit_norm$savings_balance = credit$savings_balance
credit_norm$employment_duration = credit$employment_duration
credit_norm$other_credit = credit$other_credit
credit_norm$housing = credit$housing
credit_norm$job = credit$job
credit_norm$phone = credit$phone
```
```{r}
set.seed(123)
train_sample <- sample(1000, 900)
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]
```

2- (2pt) Train a logistic regression model on the train data using the glm function and use it to predict the default for the test data

```{r}
logistic_model = glm(default~., data = credit_train, family = "binomial")
summary(logistic_model)
logistic_predict = predict(logistic_model, credit_test, type = "response")
```

3- (2pt)Compare the predictions with the actual default labels in the test data. What is the false positive rate and false negative rate of your model? How does your logistic regression model compare with the classification models we used in week 6 lectures?

```{r}
hist(logistic_predict)
```

```{r}
predicted.label = factor(ifelse(logistic_predict > 0.5, "no", "yes"))
actual.label = credit_test$default
t = table(predicted.label, actual.label)
t
error = (t[1,2] + t[2,1])/sum(t)
print(paste("the error is" ,formatC(error*100,digits = 3),"%"))

fpr = (t[1,2]/(t[1,2] + t[2,2]))
print(paste("false positive rate:",formatC(fpr*100,digits = 3),"%"))

fnr = (t[2,1]/(t[2,1]+t[2,2]))
print(paste("false negative rate:",formatC(fnr*100,digits = 3),"%"))
```

With an error of 74% I'd say that this model is considerably worse at predicting defaults that the model used in lecture 6 with its error rate of 27%

4- (6pt) The outcome variable “default” is imbalanced that is, the number of observations with default=NO is double the number of observations with default=Yes. Most classification models trained on imbalanced data are biased towards predicting the majority class and yield a higher classification error on the minority class. One way to deal with class imbalance problem is to downsample the majority class; meaning randomly sample the observations in the majority class to make it the same size as the minority class. While this approach might work for larger dataset with large enough samples in the minority class, for smaller dataset, it will result in loss of information. Another approach to deal with class imbalance is to use oversampling; meaning generating syntactic samples and add it to the minority class to make it the same size as the majority class. A popular method for generating syntactic samples is called SMOTE (Syntactic Minority Oversampling Technique) which uses KNN to generate syntactic samples for the minority class. Please watch this short Youtube video to understand SMOTE, Then install DMwR package in R and use the SMOTE function to generate syntactic training data for the minority class. Please note that oversampling or under-sampling should only be applied to the training data and the testing data should be untouched. Use perc.over=100 argument in the SMOTE function and leave the other parameters as default to create a balanced training data. Then train a logistic regression model on this balanced data and evaluate it on your test data. Compare the False Positive and the False Negative Rates of the model trained on the balanced data to your previous model trained on the imbalance data and discuss the differences.

```{r}
install.packages("DMwR")
library(DMwR)
```
```{r}
set.seed(123)
credit_smote = SMOTE(default ~ ., data = credit_train, perc.over = 100)
log_model_smote = glm(default~., data = credit_smote, family = "binomial")
summary(log_model_smote)
log_predict_smote = predict(log_model_smote, credit_test, type = "response")
```

```{r}
predicted.label = factor(ifelse(log_predict_smote > 0.5, "no", "yes"))
actual.label = credit_test$default
t = table(predicted.label, actual.label)
t
error = (t[1,2] + t[2,1])/sum(t)
print(paste("the error is" ,formatC(error*100,digits = 3),"%"))

fpr = (t[1,2]/(t[1,2] + t[2,2]))
print(paste("false positive rate:",formatC(fpr*100,digits = 3),"%"))

fnr = (t[2,1]/(t[2,1]+t[2,2]))
print(paste("false negative rate:",formatC(fnr*100,digits = 3),"%"))
```
The fpr and fnr are higher for the smote model than the model trained with the unmodified training data however the error is a bit less. Overall there isn't really an improvement with smote data over the original data. I suppose varying perc.over and perc.under until better results are yielded would be a better way to go about improving model performance (this is assuming you're being mindful not to overfit)
